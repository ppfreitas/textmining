{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:51:04.038999Z",
     "start_time": "2020-06-16T16:50:57.930407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweet-preprocessor in /opt/anaconda3/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already up-to-date: emoji in /opt/anaconda3/lib/python3.7/site-packages (0.5.4)\n",
      "Requirement already satisfied: contractions in /opt/anaconda3/lib/python3.7/site-packages (0.0.25)\n",
      "Requirement already satisfied: textsearch in /opt/anaconda3/lib/python3.7/site-packages (from contractions) (0.0.17)\n",
      "Requirement already satisfied: pyahocorasick in /opt/anaconda3/lib/python3.7/site-packages (from textsearch->contractions) (1.4.0)\n",
      "Requirement already satisfied: Unidecode in /opt/anaconda3/lib/python3.7/site-packages (from textsearch->contractions) (1.1.1)\n",
      "Requirement already satisfied: textblob in /opt/anaconda3/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/anaconda3/lib/python3.7/site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "# libraries installation\n",
    "!pip install tweet-preprocessor\n",
    "!pip install emoji --upgrade\n",
    "!pip install contractions\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:51:07.192187Z",
     "start_time": "2020-06-16T16:51:07.189108Z"
    }
   },
   "outputs": [],
   "source": [
    "# basic libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading (with tweets_sample dataset start from clean tweets section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 538 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRAhandle_tweets_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRAhandle_tweets_11.csv\n",
      "IRAhandle_tweets_9.csv\n",
      "IRAhandle_tweets_2.csv\n",
      "IRAhandle_tweets_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (0,15,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRAhandle_tweets_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (10,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRAhandle_tweets_8.csv\n",
      "IRAhandle_tweets_1.csv\n",
      "IRAhandle_tweets_6.csv\n",
      "IRAhandle_tweets_7.csv\n",
      "IRAhandle_tweets_4.csv\n",
      "IRAhandle_tweets_13.csv\n",
      "IRAhandle_tweets_3.csv\n"
     ]
    }
   ],
   "source": [
    "#read all csv in folder data\n",
    "data_dir = '../data/'\n",
    "start = True\n",
    "\n",
    "# what percentage of each csv of tweets you want in your data (1% should give us aroun 100K tweets)\n",
    "#fraction = .2\n",
    "\n",
    "\n",
    "for root,dirs,files in os.walk(data_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            print(file)\n",
    "            f = pd.read_csv(data_dir+file)\n",
    "            #f = f.sample(frac = fraction)\n",
    "            if start is True:\n",
    "                data = f\n",
    "                start = False\n",
    "            else:\n",
    "                data=data.append(f, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 2116866\n"
     ]
    }
   ],
   "source": [
    "#keep only english tweets\n",
    "\n",
    "data = data[data.language=='English']\n",
    "data = data.dropna(axis=0,subset=['content'])\n",
    "print(f'Number of tweets: {data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_orig = data\n",
    "\n",
    "# keep only original tweets (if you uncomment lines below)\n",
    "\n",
    "#tweets_orig = data[data.post_type!='RETWEET']\n",
    "#tweets_orig = tweets_orig[tweets_orig.post_type!='QUOTE_TWEET']\n",
    "#tweets_orig = tweets_orig.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of original tweets: 2116866\n"
     ]
    }
   ],
   "source": [
    "print(f'number of original tweets: {tweets_orig.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['external_author_id', 'author', 'content', 'region', 'language',\n",
       "       'publish_date', 'harvested_date', 'following', 'followers', 'updates',\n",
       "       'post_type', 'account_type', 'retweet', 'account_category',\n",
       "       'new_june_2018', 'alt_external_id', 'tweet_id', 'article_url',\n",
       "       'tco1_step1', 'tco2_step1', 'tco3_step1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_orig = tweets_orig[tweets_orig.account_category!='NonEnglish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_orig.to_csv('tweets_sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:51:13.946885Z",
     "start_time": "2020-06-16T16:51:13.945090Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the functions for preprocessing\n",
    "import tweets_preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:51:14.439692Z",
     "start_time": "2020-06-16T16:51:14.434821Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/francesco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# list of symbols to remove\n",
    "my_symbols = list(string.punctuation)\n",
    "my_symbols.remove(\"'\")\n",
    "my_symbols.append('…')\n",
    "my_symbols.append('»')\n",
    "my_symbols.append('«')\n",
    "my_symbols.append('„')\n",
    "my_symbols.append('“')\n",
    "\n",
    "# load stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# load all stop words in the three languages\n",
    "all_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_orig = tweets_orig.dropna(axis=0,subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:51:32.584933Z",
     "start_time": "2020-06-16T16:51:15.351644Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jIgEWbgU9kPF"
   },
   "outputs": [],
   "source": [
    "# clean the tweets, returns tweets_words (list of lists, each word in each tweet)\n",
    "# hashtags (list of hashtags in each tweet) \n",
    "tweets_words, hashtags, mentions, emojis = tweets_preprocessing.preprocessor(tweets_orig.content, \n",
    "                                                                             my_symbols,\n",
    "                                                                             all_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_hash = 'hillaryforpr, imwithher2016, imwithhillary, hillaryforpresident, hillaryforamerica, hereiamwithher, estoyconella, hillarysopresidential, uniteblue, hillstorm2016,bluewave2016, welovehillary, itrusther, bluewave, hrcisournominee, itrusthillary, standwithmadampotus, momsdemandhillary, madamepresident, madampresident, imwither, herstory, republicans4hillary, hillarysoqualified, werewithher, vote4hillary, strongertogether, readyforhillary, hillafornia, unitedagainsthate, votehillary, wearewithher, republicansforhillary, hrc2016, connecttheleft, yeswekaine, voteblue2016, hillary2016, sheswithus, hillyes, iamwithher, heswithher, voteblue, hillaryaprovenleader, imwiththem, bernwithher, ohhillyes, imwithher, clintonkaine2016, whyimwithher, turnncblue, hillarystrong'\n",
    "positive_hash = positive_hash.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_hash = set(['#'+has for has in positive_hash]+['#gohillary',\n",
    " '#hillaryclinton16',\n",
    " '#hillaryclintonforpresident',\n",
    " '#ilovehillary',\n",
    " '#republicansforhillary',\n",
    " '#readyforhillary',\n",
    " '#votehillary',\n",
    " '#girls4hillary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_hash = 'hillarysolympics, hillaryforprison, hillaryforprison2016, moretrustedthanhillary, heartlesshillary, neverclinton, handcuffhillary, queenofcorrupton, crookedhiliary, nomoreclintons, hillary4jail, fbimwithher, clintoncrimefamily, hillno, queenofcorruption, hillarylostme, ohhillno, billclintonisrapist, democratliesmatter, lyingcrookedhillary, hypocritehillary, crookedclintons, hillarylies, neverhilary, shelies, releasethetranscripts, stophillary2016, riskyhillary, hillaryliedpeopledied, corrupthillary, hillary4prison2016, nohillary2016, wehatehillary, whatmakeshillaryshortcircuit, crookedhillaryclinton, deletehillary, dropouthillary, lyinhillary, hillaryliesmatter, nevereverhillary, stophillary, neverhilllary, clintoncorruption, clintoncrime, notwithher, hillary2jail, imnotwithher, lockherup, corruptclintons, indicthillary, sickhillary, crookedhilary, crookedhillary, hillaryrottenclinton, theclintoncontamination, lyinghillary, clintoncollapse, clintoncrimefoundation, neverhillary, criminalhillary, crookedclinton, hillary4prison, killary, iwillneverstandwithher'\n",
    "negative_hash = negative_hash.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_hash = set(['#'+has for has in negative_hash] + ['#neverhillary',\n",
    " '#thingsmoretrustedthanhillary',\n",
    " '#crookedhillary',\n",
    " '#makeamoviehillary',\n",
    " '#sickhillary',\n",
    " '#thingshillarygoogles',\n",
    " '#zombiehillary',\n",
    " '#indicthillary',\n",
    " '#dropouthillary',\n",
    " '#releaseclintonsmedicalrecords',\n",
    " '#whyimnotvotingforhillary',\n",
    " '#billclintonisarapist',\n",
    " '#losinglikehillary',\n",
    " '#stophillary2016',\n",
    " '#herpeshillary',\n",
    " '#thingshillarywillneverhave',\n",
    " '#neverclinton',\n",
    " '#betternamesforhillarysbook',\n",
    " '#corrupthillary',\n",
    " '#nohillary',\n",
    " '#anybodybutclinton',\n",
    " '#noclintonsever',\n",
    " '#heilhillary',\n",
    " '#handcuffhillary',\n",
    " '#whatclintonwrites',\n",
    " '#nohillary2016',\n",
    " '#hypocritehillary',\n",
    " '#nevercrookedhillary',\n",
    " '#dontkillmehillary',\n",
    " '#billclintonrapist',\n",
    " '#3wordhillary',\n",
    " '#pedophilesforhillary',\n",
    " '#sexoffendersforhillary',\n",
    " '#satanists4hillary',\n",
    " '#arresthillary',\n",
    " '#lesshillarymorefun',\n",
    " '#libertynothillary',\n",
    " '#crookedclinton',\n",
    " '#assad_clinton',\n",
    " '#stopclinton',\n",
    " '#deplorablehillary',\n",
    " '#hillaryrottenclinton',\n",
    " '#saynotohillary',\n",
    " '#rejectedhillaryslogans',\n",
    " '#lyincrookedhillary',\n",
    " '#censoredforhillary',\n",
    " '#corruptclintons',\n",
    " '#unforgivablehillary',\n",
    " '#madhillary',\n",
    " '#whathillarywouldforadollar',\n",
    " '#impeachhillary',\n",
    " '#abusivehillary',\n",
    " '#stophillary',\n",
    " '#hackinghillary',\n",
    " '#arrestclinton',\n",
    " '#murderinghillary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hash = positive_hash.union(negative_hash)\n",
    "hillary_clinton = set(['hillary','clinton'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'#hillary4prison', '#hillaryliedpeopledied', '#hillaryrottenclinton', '#satanists4hillary', '#deplorablehillary', '#3wordhillary', '#censoredforhillary', '#releaseclintonsmedicalrecords', '#madhillary', '#hillaryforprison2016', '#hypocritehillary', '#imnotwithher', '#whyimnotvotingforhillary', '#lyinghillary', '#criminalhillary', '#crookedclinton', '#heartlesshillary', '#unforgivablehillary', '#thingshillarywillneverhave', '#hillarylostme', '#moretrustedthanhillary', '#crookedhilary', '#sickhillary', '#rejectedhillaryslogans', '#queenofcorruption', '#dropouthillary', '#shelies', '#crookedhiliary', '#notwithher', '#queenofcorrupton', '#lyincrookedhillary', '#neverclinton', '#nevercrookedhillary', '#wehatehillary', '#clintoncrime', '#lockherup', '#democratliesmatter', '#arrestclinton', '#arresthillary', '#impeachhillary', '#corruptclintons', '#indicthillary', '#lyingcrookedhillary', '#nomoreclintons', '#deletehillary', '#handcuffhillary', '#hackinghillary', '#libertynothillary', '#nohillary2016', '#zombiehillary', '#anybodybutclinton', '#corrupthillary', '#sexoffendersforhillary', '#crookedclintons', '#thingsmoretrustedthanhillary', '#hillaryforprison', '#assad_clinton', '#betternamesforhillarysbook', '#saynotohillary', '#billclintonrapist', '#pedophilesforhillary', '#thingshillarygoogles', '#crookedhillaryclinton', '#dontkillmehillary', '#nohillary', '#whatmakeshillaryshortcircuit', '#clintoncrimefamily', '#hillary4prison2016', '#hillarylies', '#makeamoviehillary', '#theclintoncontamination', '#neverhillary', '#iwillneverstandwithher', '#fbimwithher', '#neverhilllary', '#whatclintonwrites', '#herpeshillary', '#clintoncollapse', '#stophillary2016', '#hillno', '#crookedhillary', '#billclintonisrapist', '#ohhillno', '#clintoncrimefoundation', '#clintoncorruption', '#noclintonsever', '#releasethetranscripts', '#stopclinton', '#abusivehillary', '#lesshillarymorefun', '#murderinghillary', '#riskyhillary', '#billclintonisarapist', '#whathillarywouldforadollar', '#hillary4jail', '#killary', '#hillary2jail', '#stophillary', '#lyinhillary', '#heilhillary', '#hillarysolympics', '#hillaryliesmatter', '#neverhilary', '#losinglikehillary', '#nevereverhillary'}\n"
     ]
    }
   ],
   "source": [
    "print(negative_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_list = []\n",
    "for h in hashtags:\n",
    "    hashtags_list.append(h.lower().split())\n",
    "    \n",
    "hillary_tweets = []\n",
    "hillary_hashtags = []\n",
    "hillary_tweet_meta = []\n",
    "for tw,hashs,au,date,retweet,content in zip(tweets_words,hashtags_list,tweets_orig.author,tweets_orig.publish_date,tweets_orig.retweet,tweets_orig.content):\n",
    "    j = len(set(tw) & hillary_clinton) + len(set(hashs) & all_hash)\n",
    "    if j > 0:\n",
    "        hillary_tweets.append(re.sub(\"[']\", '',' '.join(tw)))\n",
    "        hillary_hashtags.append(hashs)\n",
    "        hillary_tweet_meta.append([au,re.sub(\"/\", ' ',date),retweet,content])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm(doc):\n",
    "    words = [t.lemma_ for t in nlp(doc) if t.lemma_ != '-PRON-']\n",
    "    return ' '.join(words).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=10, preprocessor = lemm)\n",
    "tf = tf_vectorizer.fit_transform(hillary_tweets)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "sanders president bernie got obama need would million today meeting run dead assange flashback iowa\n",
      "Topic 1:\n",
      "american fake bill good trumps bush thats crimes thinker give north days needs yet weinstein\n",
      "Topic 2:\n",
      "says know make remember well day next gets every nothing law gave votes kkk wanted\n",
      "Topic 3:\n",
      "breaking comey one amp like election lynch win big let help money lies democratic people\n",
      "Topic 4:\n",
      "fbi russia white investigation look house congress email report probe doj latest emails much corrupt\n",
      "Topic 5:\n",
      "get still time back ever bad even old gun john wall guns ohio revealed gives\n",
      "Topic 6:\n",
      "debate wants email wikileaks cnn used via prison dnc party secret server team top private\n",
      "Topic 7:\n",
      "vote trump say im take stop really must way away attack says come war anyone\n",
      "Topic 8:\n",
      "bill trump deal chelsea via cant support corruption gop uranium speech rally foundation candidate show\n",
      "Topic 9:\n",
      "rt emails state clintons benghazi may made claims national says department dept scandal called security\n",
      "Topic 10:\n",
      "trump poll go never voters voting race dems criminal many stupid lead tells leads thinks\n",
      "Topic 11:\n",
      "foundation people would it primary donor sick democrat case kaine tax tarmac james admits lynch\n",
      "Topic 12:\n",
      "new campaign trump supporters women lol news lost election political republicans blaming foreign book fraud\n",
      "Topic 13:\n",
      "trump obama donald us america crooked calls going want see americans another country loss hate\n",
      "Topic 14:\n",
      "video media black watch think right judge woman lying states shows special sessions great please\n"
     ]
    }
   ],
   "source": [
    "no_topics = 15\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 15\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "hillary amp clinton want voters trump big see let wikileaks americans another country dems loss\n",
      "Topic 1:\n",
      "clinton people foundation say hillary it need show rally bill sessions dead left old pay\n",
      "Topic 2:\n",
      "hillary clinton end we turn msm biden telling feel killed least re line reports joe\n",
      "Topic 3:\n",
      "clinton back bill top stupid way scandal north nuclear gun lied sex office korea started\n",
      "Topic 4:\n",
      "hillary breaking trump supporters know got even doj lies race presidential million video meeting come\n",
      "Topic 5:\n",
      "clinton hillary trump crooked debate russia money deal chelsea foundation uranium speech obama cant bad\n",
      "Topic 6:\n",
      "hillary still corruption right may clinton much state national lying department hate goes special great\n",
      "Topic 7:\n",
      "hillary us going well next really trumps real every today twitter thats clinton needs muslim\n",
      "Topic 8:\n",
      "hillary clinton good times better collusion weinstein lynch tarmac trust reason loretta hope would working\n",
      "Topic 9:\n",
      "clinton says obama hillary win state support crimes dept bill assange paid flashback last give\n",
      "Topic 10:\n",
      "hillary fake lol via im used caught clinton also work enough child sure wait husband\n",
      "Topic 11:\n",
      "hillary clinton trump video election donald media watch news obama black america think woman via\n",
      "Topic 12:\n",
      "clinton new fbi comey email hillary lynch investigation make congress via probe latest run gave\n",
      "Topic 13:\n",
      "rt hillary shows jail free saying told hell guns revealed republican illegal went flopping aces\n",
      "Topic 14:\n",
      "hillary wants first gets day nothing hillarys days kkk sexual bring sign general everyone hey\n",
      "Topic 15:\n",
      "mueller anything case dirty change talk wow believes clinton start ass campaigns high antifa st\n",
      "Topic 16:\n",
      "hillary sanders clinton like one bernie vote go trump voting said never take judge could\n",
      "Topic 17:\n",
      "hillary clinton campaign emails get clintons time calls benghazi ever trump report blaming fire tells\n",
      "Topic 18:\n",
      "clinton trump poll gop stop states hillary vs tell potus leads face plan ahead ties\n",
      "Topic 19:\n",
      "clinton hillary bill would president white women look house american remember obama made democratic former\n"
     ]
    }
   ],
   "source": [
    "no_topics = 20\n",
    "\n",
    "# Run LDA\n",
    "lda2 = LatentDirichletAllocation(n_components=no_topics, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 15\n",
    "display_topics(lda2, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "hillary clinton bill obama one use call white report well house say good remember watch\n",
      "Topic 1:\n",
      "hillary clinton vote trump not obama want go election like win rt say would get\n",
      "Topic 2:\n",
      "clinton hillary trump sander video poll lie bill donald voter tell campaign time say try\n",
      "Topic 3:\n",
      "hillary clinton trump know give supporter campaign american pay fake right corruption work thing claim\n",
      "Topic 4:\n",
      "clinton hillary email rt new break fbi comey foundation state russia lynch investigation doj via\n"
     ]
    }
   ],
   "source": [
    "no_topics = 5\n",
    "\n",
    "# Run LDA\n",
    "lda3 = LatentDirichletAllocation(n_components=no_topics, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 15\n",
    "display_topics(lda3, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=LatentDirichletAllocation(batch_size=128,\n",
       "                                                 doc_topic_prior=None,\n",
       "                                                 evaluate_every=-1,\n",
       "                                                 learning_decay=0.7,\n",
       "                                                 learning_method='batch',\n",
       "                                                 learning_offset=10.0,\n",
       "                                                 max_doc_update_iter=100,\n",
       "                                                 max_iter=10,\n",
       "                                                 mean_change_tol=0.001,\n",
       "                                                 n_components=10, n_jobs=None,\n",
       "                                                 perp_tol=0.1,\n",
       "                                                 random_state=None,\n",
       "                                                 topic_word_prior=None,\n",
       "                                                 total_samples=1000000.0,\n",
       "                                                 verbose=0),\n",
       "             iid='warn', n_jobs=5,\n",
       "             param_grid={'doc_topic_prior': [0.001, None],\n",
       "                         'n_components': [5, 10, 15, 20, 25, 30, 35]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [5, 10, 15, 20, 25, 30, 35], 'doc_topic_prior':[0.001,None]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params, n_jobs = 5)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'doc_topic_prior': None, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -954065.2808361806\n",
      "Model Perplexity:  932.3007538079268\n"
     ]
    }
   ],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Use Gensim's NMF to get the best num of topics via coherence score\n",
    "texts = [tw.split() for tw in hillary_tweets]\n",
    "\n",
    "# Create a dictionary\n",
    "# In gensim a dictionary is a mapping between words and their integer id\n",
    "dictionary = Dictionary(texts)\n",
    "\n",
    "# Filter out extremes to limit the number of features\n",
    "dictionary.filter_extremes(\n",
    "    no_below=3,\n",
    "    no_above=0.85,\n",
    "    keep_n=5000\n",
    ")\n",
    "\n",
    "# Create the bag-of-words format (list of (token_id, token_count))\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Create a list of the topic numbers we want to try\n",
    "topic_nums = list(np.arange(5, 75 + 1, 5))\n",
    "\n",
    "# Run the nmf model and calculate the coherence score\n",
    "# for each number of topics\n",
    "coherence_scores = []\n",
    "\n",
    "for num in topic_nums:\n",
    "    nmf = Nmf(\n",
    "        corpus=corpus,\n",
    "        num_topics=num,\n",
    "        id2word=dictionary,\n",
    "        chunksize=2000,\n",
    "        passes=5,\n",
    "        kappa=.1,\n",
    "        minimum_probability=0.01,\n",
    "        w_max_iter=300,\n",
    "        w_stop_condition=0.0001,\n",
    "        h_max_iter=100,\n",
    "        h_stop_condition=0.001,\n",
    "        eval_every=10,\n",
    "        normalize=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Run the coherence model to get the score\n",
    "    cm = CoherenceModel(\n",
    "        model=nmf,\n",
    "        texts=texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    \n",
    "    coherence_scores.append(round(cm.get_coherence(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'itemgetter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-8f0a2ebfba75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Get the number of topics with the highest coherence score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_nums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mbest_num_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_num_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'itemgetter' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the number of topics with the highest coherence score\n",
    "scores = list(zip(topic_nums, coherence_scores))\n",
    "best_num_topics = sorted(scores, key=itemgetter(1), reverse=True)[0][0]\n",
    "\n",
    "print(best_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n    n_components=None, random_state=None, shuffle=False, solver='cd',\n    tol=0.0001, verbose=0) does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-9eeb1ffbd812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Do the Grid Search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel_nmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         scorers, self.multimetric_ = _check_multimetric_scoring(\n\u001b[0;32m--> 608\u001b[0;31m             self.estimator, scoring=self.scoring)\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultimetric_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m_check_multimetric_scoring\u001b[0;34m(estimator, scoring)\u001b[0m\n\u001b[1;32m    340\u001b[0m     if callable(scoring) or scoring is None or isinstance(scoring,\n\u001b[1;32m    341\u001b[0m                                                           str):\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0;34m\"If no scoring is specified, the estimator passed should \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0;34m\"have a 'score' method. The estimator %r does not.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 % estimator)\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         raise ValueError(\"For evaluating multiple scores, use \"\n",
      "\u001b[0;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n    n_components=None, random_state=None, shuffle=False, solver='cd',\n    tol=0.0001, verbose=0) does not."
     ]
    }
   ],
   "source": [
    "search_params = {'n_components': [5, 10, 15, 20, 25, 30], 'alpha':[0.001,.01,.1]}\n",
    "\n",
    "# Init the Model\n",
    "nmf = NMF()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model_nmf = GridSearchCV(nmf, param_grid=search_params, n_jobs = 5)\n",
    "\n",
    "# Do the Grid Search\n",
    "model_nmf.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "trump donald poll supporters debate russia vs says voters leads lead win rally wins like\n",
      "Topic 1:\n",
      "obama comey lynch fbi calls mueller congress house crimes america russia meeting congressman investigation abedin\n",
      "Topic 2:\n",
      "breaking doj deal plea offers expanding probe rumored take report video crooked grounds ample prosecuting\n",
      "Topic 3:\n",
      "sanders bernie debate iowa says democratic supporters campaign primary race voters clash california fight win\n",
      "Topic 4:\n",
      "bill remember campaign used hate flashback symbol made fire fury comments allwhite club joined golf\n",
      "Topic 5:\n",
      "vote women people didn black voting popular would want im never get please said america\n",
      "Topic 6:\n",
      "emails state foundation fbi email department via clintons dept release campaign deleted investigation wikileaks benghazi\n",
      "Topic 7:\n",
      "rt amp debalwaystrump https russia america realdonaldtrump rr mueller one think republicans nuclear benghazi would\n",
      "Topic 8:\n",
      "president qualified would never next one want run like become endorses becomes woman think obama\n",
      "Topic 9:\n",
      "new poll york hampshire excuse shows leads go voting lol lead points book brand fraud\n",
      "Topic 10:\n",
      "election us going interference ukraine loss know media wants still win results lost blaming would\n",
      "Topic 11:\n",
      "video watch judge sessions napolitano explain it kkk mentor reminder jeanine calls lol go thinks\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words = ['hillary','clinton'])\n",
    "tfidf = tfidf_vectorizer.fit_transform(hillary_tweets)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "no_topics=12\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=0, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_share = np.matrix(nmf.transform(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = np.argmax(topic_share, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(results.author))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(hillary_tweet_meta, columns = ['author','date','is_retweet','content'])\n",
    "results['hashtags'] = [' '.join(tw_hashtags) for tw_hashtags in hillary_hashtags]\n",
    "results['topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "results.date = [datetime.strptime(d, '%m %d %Y %H:%M') for d in results.date]\n",
    "results['year']=[d.year for d in results.date]\n",
    "results['month']=[d.month for d in results.date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_grouped = results.groupby(['topic','month','year'], as_index=False)['author'].count()\n",
    "res_total = results.groupby(['month','year'], as_index=False)['author'].count()\n",
    "\n",
    "results = res_grouped.merge(res_total, on = ['month','year'])\n",
    "results['total_tweets'] = results.author_x\n",
    "results['tweets_share'] = results.author_x/results.author_y\n",
    "\n",
    "del results['author_x']\n",
    "del results['author_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = pd.DataFrame([[topic, month, year] for topic in range(no_topics) \n",
    "                                 for month in range(1,13) \n",
    "                                 for year in [2015,2016,2017,2018]], \n",
    "                                columns = ['topic','month','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.merge(all_combinations, on=['topic','month','year'], how='outer')\n",
    "results = results.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['yearmonth'] = (results.year-2015)*12 + results.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=['yearmonth','topic'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('../results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(hillary_tweet_meta, columns = ['author','date','is_retweet','content'])\n",
    "results['hashtags'] = [' '.join(tw_hashtags) for tw_hashtags in hillary_hashtags]\n",
    "results['topic'] = topics\n",
    "\n",
    "deff = np.array([1 for _ in range(len(results))])\n",
    "deff[results.topic == 0] = 2\n",
    "deff[results.topic == 2] = 2\n",
    "deff[results.topic == 6] = 2\n",
    "deff[results.topic == 5] = 0\n",
    "deff[results.topic == 8] = 0\n",
    "\n",
    "results['deff'] = deff\n",
    "results = results[results.deff != 1]\n",
    "\n",
    "res_grouped = results.groupby(['author','deff'], as_index=False)['topic'].count()\n",
    "res_total = results.groupby(['author'], as_index=False)['topic'].count()\n",
    "\n",
    "results = res_grouped.merge(res_total, on = ['author'])\n",
    "results['tweets_share'] = results.topic_x/results.topic_y\n",
    "\n",
    "results = results[results.topic_y > 25]\n",
    "\n",
    "del results['topic_x']\n",
    "del results['topic_y']\n",
    "\n",
    "results.to_csv('../def_by_author.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(hillary_tweet_meta, columns = ['author','date','is_retweet','content'])\n",
    "results['hashtags'] = [' '.join(tw_hashtags) for tw_hashtags in hillary_hashtags]\n",
    "results['topic'] = topics\n",
    "\n",
    "\n",
    "res_grouped = results.groupby(['topic','is_retweet'], as_index=False)['author'].count()\n",
    "res_total = results.groupby(['topic'], as_index=False)['author'].count()\n",
    "\n",
    "results = res_grouped.merge(res_total, on = ['topic'])\n",
    "results['tweets_share'] = results.author_x/results.author_y\n",
    "\n",
    "results = results[results.author_y > 25]\n",
    "\n",
    "del results['author_x']\n",
    "del results['author_y']\n",
    "\n",
    "results.to_csv('../ret_by_topic.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
